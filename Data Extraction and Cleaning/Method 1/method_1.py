# -*- coding: utf-8 -*-
"""Method 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AOQVAsY1KxWw2KPyJTmUIggQhMFEQAmC

# **Extract & Clean Data**
"""

import os
os.listdir("/content")

pdf_path = "/content/ACt-vs-CBT-for-Anxiety.pdf.pdf"

!pip install pdfminer.six docx transformers sentence-transformers

!pip install pdfplumber

import pdfplumber
import pandas as pd

!pip install camelot-py

# Extract text and tables from the new meta-analysis PDF
import pdfplumber

# Path to the new uploaded PDF
pdf_path = "/content/ACt-vs-CBT-for-Anxiety.pdf"

# Initialize storage for text and tables
extracted_text = []
extracted_tables = []

# Open the PDF and extract text and tables
with pdfplumber.open(pdf_path) as pdf:
    for page in pdf.pages:
        # Extract text from each page
        text = page.extract_text()
        if text:
            extracted_text.append(text)

        # Extract tables from each page
        tables = page.extract_tables()
        if tables:
            extracted_tables.extend(tables)

# Combine extracted text into one cleaned string
cleaned_text = "\n".join(extracted_text)

# Save text as Markdown file
md_file_path = "/content/ACT_vs_CBT_cleaned.md"
with open(md_file_path, "w") as f:
    f.write(cleaned_text)

import camelot
import pandas as pd
from google.colab import files

# Path to the uploaded PDF
pdf_path = "/content/ACt-vs-CBT-for-Anxiety.pdf"

# Extract tables using Camelot's "stream" mode
tables = camelot.read_pdf(pdf_path, pages="all", flavor="stream")

# Check how many tables were detected
print(f"Total tables detected: {tables.n}")

if tables.n > 0:
    extracted_tables = [tables[i].df for i in range(tables.n)]
    combined_table_df = pd.concat(extracted_tables, ignore_index=True)

    # **Remove non-table content by setting a length threshold**
    max_text_length = 100  # Adjust if needed
    df_cleaned = combined_table_df[combined_table_df.apply(lambda row: row.astype(str).str.len().max() < max_text_length, axis=1)]

    # Save cleaned tables as CSV
    csv_file_path = "/content/ACT_vs_CBT_cleaned_tables.csv"
    df_cleaned.to_csv(csv_file_path, index=False)

    print(f"Cleaned tables saved at: {csv_file_path}")

    # Download the cleaned CSV
    files.download(csv_file_path)
else:
    print("No tables detected.")

import pandas as pd
import re
from google.colab import files

# Step 1: Load the cleaned CSV file (already extracted using Camelot)
file_path = "/content/ACT_vs_CBT_cleaned_tables.csv"
df = pd.read_csv(file_path, header=None, encoding='utf-8', on_bad_lines='skip')

# Step 2: Remove rows containing unwanted text patterns
unwanted_patterns = [
    "doi:", "DOI", "Retrieved from", "References", "Appendix",
    "Supplemental materials", "This article was published", "E-mail"
]

df = df[~df.apply(lambda row: row.astype(str).str.contains('|'.join(unwanted_patterns), case=False, na=False).any(), axis=1)]

# Step 3: Filter out rows that are not structured tables
def is_table_row(row):
    """Check if a row is likely part of a table (has at least 2 numbers)."""
    text = ' '.join(map(str, row)).strip()
    num_count = len(re.findall(r'\d+', text))  # Count numeric values
    return num_count >= 2  # Keep only rows with at least 2 numbers

df_filtered = df[df.apply(is_table_row, axis=1)]

# Step 4: Fix encoding issues and clean text
def clean_text(text):
    """Fixes encoding issues and removes unwanted characters."""
    if isinstance(text, str):
        text = text.replace("â€”", "—")  # Fixes dashes
        text = text.replace("\n", " ")  # Removes line breaks
        text = re.sub(r'\s+', ' ', text).strip()  # Normalize spaces
    return text

df_filtered = df_filtered.applymap(clean_text)

# Step 5: Save the final cleaned table
final_csv_path = "/content/ACT_vs_CBT_cleaned_table_2.csv"
df_filtered.to_csv(final_csv_path, index=False, encoding='utf-8')

print(f"✅ Fully cleaned tables saved at: {final_csv_path}")

# Step 6: Provide a download link for the final cleaned CSV
files.download(final_csv_path)

import pandas as pd
import re
from google.colab import files

# Load the cleaned CSV file
file_path = "/content/ACT_vs_CBT_cleaned_table_2.csv"
df = pd.read_csv(file_path, header=None, encoding='utf-8', on_bad_lines='skip')

# Function to check if a row belongs to a table (at least 2 numeric values)
def is_table_row(row):
    """Check if a row is part of a table by detecting at least 2 numbers."""
    text = ' '.join(map(str, row)).strip()
    num_count = len(re.findall(r'\d+', text))
    return num_count >= 2  # True if at least 2 numbers exist

# Create a boolean column to mark table rows
df["is_table"] = df.apply(is_table_row, axis=1)

# Identify title rows (row before a table)
df["is_title"] = False  # Default to False
table_indices = df.index[df["is_table"]].tolist()
for idx in table_indices:
    if idx > 0:  # Avoid first-row issues
        df.at[idx - 1, "is_title"] = True  # Mark title row

# Keep only tables and their title rows
df_filtered = df[df["is_table"] | df["is_title"]].drop(columns=["is_table", "is_title"], errors="ignore")

# Save the cleaned CSV
final_csv_path = "/content/ACT_vs_CBT_cleaned_table_3.csv"
df_filtered.to_csv(final_csv_path, index=False, encoding='utf-8')

print(f"✅ Final structured tables saved at: {final_csv_path}")

# Download the cleaned CSV
files.download(final_csv_path)

!pip install pymupdf

import pandas as pd
import fitz  # PyMuPDF for PDF processing
import re
from google.colab import files

# Upload the files manually or provide paths
pdf_path = "/content/ACt-vs-CBT-for-Anxiety.pdf"
csv_path = "/content/ACT_vs_CBT_cleaned_table_3.csv"  # Your cleaned CSV file

# Load CSV
df = pd.read_csv(csv_path, header=None, encoding='utf-8', on_bad_lines='skip')

# Extract text from PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    doc = fitz.open(pdf_path)
    for page in doc:
        text += page.get_text("text") + "\n"
    return text

pdf_text = extract_text_from_pdf(pdf_path)

# Extract table titles using regex (assumes "Table X: Title" format)
table_titles = re.findall(r"(Table\s\d+:.*)", pdf_text)

# Function to check if a row is part of a table (at least 2 numeric values)
def is_table_row(row):
    text = ' '.join(map(str, row)).strip()
    num_count = len(re.findall(r'\d+', text))
    return num_count >= 2  # True if at least 2 numbers exist

# Detect table rows
df["is_table"] = df.apply(is_table_row, axis=1)

# Identify potential title rows (row before a table)
df["is_title"] = False
table_indices = df.index[df["is_table"]].tolist()

for i, idx in enumerate(table_indices):
    if idx > 0 and i < len(table_titles):  # Ensure within bounds
        df.at[idx - 1, 0] = table_titles[i]  # Assign title to first column of previous row

# Remove helper columns
df.drop(columns=["is_table", "is_title"], errors="ignore", inplace=True)

# Save cleaned file
final_csv_path = "/content/ACT_vs_CBT_cleaned_table_4.csv"
df.to_csv(final_csv_path, index=False, encoding='utf-8')

print(f"✅ Final structured tables with titles saved at: {final_csv_path}")

# Download the cleaned CSV
files.download(final_csv_path)

import camelot
import pandas as pd
import re
from google.colab import files

# 📌 Step 1: Extract Tables from the PDF
pdf_path = "/content/ACt-vs-CBT-for-Anxiety.pdf"

# Extract tables using Camelot (stream mode)
tables = camelot.read_pdf(pdf_path, pages="all", flavor="stream")

# Check how many tables were detected
print(f"Total tables detected: {tables.n}")

if tables.n > 0:
    extracted_tables = [tables[i].df for i in range(tables.n)]
    combined_table_df = pd.concat(extracted_tables, ignore_index=True)

    # **Remove non-table content by setting a length threshold**
    max_text_length = 100  # Adjust if needed
    df_cleaned = combined_table_df[
        combined_table_df.apply(lambda row: row.astype(str).str.len().max() < max_text_length, axis=1)
    ]

    # Save cleaned tables as CSV
    csv_file_path = "/content/ACT_vs_CBT_cleaned_tables_1.csv"
    df_cleaned.to_csv(csv_file_path, index=False)

    print(f"✅ Cleaned tables saved at: {csv_file_path}")

    # Download the cleaned CSV
    files.download(csv_file_path)
else:
    print("❌ No tables detected.")
    exit()

# 📌 Step 2: Load the Cleaned CSV and Filter Out Unwanted Text
file_path = "/content/ACT_vs_CBT_cleaned_tables_1.csv"
df = pd.read_csv(file_path, header=None, encoding="utf-8", on_bad_lines="skip")

# Remove rows containing unwanted text patterns
unwanted_patterns = [
    "doi:", "DOI", "Retrieved from", "References", "Appendix",
    "Supplemental materials", "This article was published", "E-mail"
]

df = df[
    ~df.apply(lambda row: row.astype(str).str.contains("|".join(unwanted_patterns), case=False, na=False).any(), axis=1)
]

# 📌 Step 3: Keep Only Table Rows (Require at Least 2 Numeric Values)
def is_table_row(row):
    """Check if a row is likely part of a table (has at least 2 numbers)."""
    text = " ".join(map(str, row)).strip()
    num_count = len(re.findall(r"\d+", text))  # Count numeric values
    return num_count >= 2  # Keep only rows with at least 2 numbers

df_filtered = df[df.apply(is_table_row, axis=1)]

# 📌 Step 4: Fix Encoding Issues and Clean Text
def clean_text(text):
    """Fix encoding issues and remove unwanted characters."""
    if isinstance(text, str):
        text = text.replace("â€”", "—")  # Fixes dashes
        text = text.replace("\n", " ")  # Removes line breaks
        text = re.sub(r"\s+", " ", text).strip()  # Normalize spaces
    return text

df_filtered = df_filtered.applymap(clean_text)

# 📌 Step 5: Save the Final Cleaned Table
final_csv_path = "/content/ACT_vs_CBT_cleaned_tables_2.csv"
df_filtered.to_csv(final_csv_path, index=False, encoding="utf-8")

print(f"✅ Fully cleaned tables saved at: {final_csv_path}")

# 📌 Step 6: Provide a Download Link for the Final Cleaned CSV
files.download(final_csv_path)

# 📌 Step 7: Load the Cleaned CSV Again for Final Processing
file_path = "/content/ACT_vs_CBT_cleaned_tables_2.csv"
df = pd.read_csv(file_path, header=None, encoding="utf-8", on_bad_lines="skip")

# Function to check if a row belongs to a table (at least 2 numeric values)
def is_table_row(row):
    """Check if a row is part of a table by detecting at least 2 numbers."""
    text = " ".join(map(str, row)).strip()
    num_count = len(re.findall(r"\d+", text))
    return num_count >= 2  # True if at least 2 numbers exist

# Create a boolean column to mark table rows
df["is_table"] = df.apply(is_table_row, axis=1)

# Identify title rows (row before a table)
df["is_title"] = False  # Default to False
table_indices = df.index[df["is_table"]].tolist()
for idx in table_indices:
    if idx > 0:  # Avoid first-row issues
        df.at[idx - 1, "is_title"] = True  # Mark title row

# Keep only tables and their title rows
df_filtered = df[df["is_table"] | df["is_title"]].drop(columns=["is_table", "is_title"], errors="ignore")

# 📌 Step 8: Save the Final Structured CSV
final_csv_path = "/content/ACT_vs_CBT_cleaned_tables_3.csv"
df_filtered.to_csv(final_csv_path, index=False, encoding="utf-8")

print(f"✅ Final structured tables saved at: {final_csv_path}")

# 📌 Step 9: Download the Final CSV
files.download(final_csv_path)

import pandas as pd

# Load the CSV file
file_path = "/content/ACT_vs_CBT_cleaned_tables_3.csv"

# Read CSV while ignoring unnecessary rows at the start
df = pd.read_csv(file_path, skip_blank_lines=True, header=None)

# Drop rows where most columns contain non-numeric or long text (removes text-heavy noise)
df_cleaned = df[~df.astype(str).apply(lambda x: x.str.contains(r'[a-zA-Z\s]{25,}', regex=True)).any(axis=1)]

# Drop rows where all values are NaN
df_cleaned = df_cleaned.dropna(how='all')

# Remove first 3 unwanted rows if they contain just "0" and "1" artifacts
if df_cleaned.iloc[:3].applymap(lambda x: str(x).strip()).isin(["0", "1"]).all(axis=1).sum() == 3:
    df_cleaned = df_cleaned.iloc[3:]

# Reset index after cleaning
df_cleaned.reset_index(drop=True, inplace=True)

# Save the cleaned data to a new CSV file
cleaned_file_path = "/content/ACT_vs_CBT_final_cleaned.csv"
df_cleaned.to_csv(cleaned_file_path, index=False, header=False)

print(f"✅ Final cleaned CSV saved to: {cleaned_file_path}")

from google.colab import drive
drive.mount('/content/drive')

pdf_dir = "/content/drive/My Drive/4th SEM Project/Meta Papers"