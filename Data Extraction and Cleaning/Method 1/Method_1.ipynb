{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Extract & Clean Data**"
      ],
      "metadata": {
        "id": "m0xOnaoBxzXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"/content\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1-SL3CMyQ2B",
        "outputId": "a20f2241-c98b-4846-827f-bf952c9987a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'ACt-vs-CBT-for-Anxiety.pdf', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/ACt-vs-CBT-for-Anxiety.pdf.pdf\""
      ],
      "metadata": {
        "id": "Sj32YdLQyXz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six docx transformers sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3fjY9wQyg6p",
        "outputId": "a40da57f-b26a-471c-deb4-6d5804679fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from docx) (5.3.1)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.11/dist-packages (from docx) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docx\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53892 sha256=56c3cc383ca94170325d5df0682d31d4301b06403d12929ffe8017ed8fa98e95\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/3e/c3/e81c11effd0be5658a035947c66792dd993bcff317eae0e1ed\n",
            "Successfully built docx\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, docx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pdfminer.six, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed docx-0.2.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pdfminer.six-20240706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJoWRgVUzCbw",
        "outputId": "9c946c00-62be-4e07-bd1c-39b38f2b04fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "  Attempting uninstall: pdfminer.six\n",
            "    Found existing installation: pdfminer.six 20240706\n",
            "    Uninstalling pdfminer.six-20240706:\n",
            "      Successfully uninstalled pdfminer.six-20240706\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "edN9o--5zO_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install camelot-py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlsg4i0Q1zQC",
        "outputId": "3dc0e759-b895-45e2-c65a-c587b706017c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting camelot-py\n",
            "  Downloading camelot_py-1.0.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.11/dist-packages (from camelot-py) (8.1.8)\n",
            "Requirement already satisfied: chardet>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py) (5.2.0)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from camelot-py) (1.26.4)\n",
            "Requirement already satisfied: openpyxl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py) (3.1.5)\n",
            "Collecting pdfminer-six>=20240706 (from camelot-py)\n",
            "  Using cached pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pypdf<4.0,>=3.17 (from camelot-py)\n",
            "  Downloading pypdf-3.17.4-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pandas>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from camelot-py) (2.2.2)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from camelot-py) (0.9.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from camelot-py) (4.11.0.86)\n",
            "Requirement already satisfied: pypdfium2>=4 in /usr/local/lib/python3.11/dist-packages (from camelot-py) (4.30.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl>=3.1.0->camelot-py) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.2->camelot-py) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer-six>=20240706->camelot-py) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer-six>=20240706->camelot-py) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->camelot-py) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py) (2.22)\n",
            "Downloading camelot_py-1.0.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "Downloading pypdf-3.17.4-py3-none-any.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf, pdfminer-six, camelot-py\n",
            "  Attempting uninstall: pdfminer-six\n",
            "    Found existing installation: pdfminer.six 20231228\n",
            "    Uninstalling pdfminer.six-20231228:\n",
            "      Successfully uninstalled pdfminer.six-20231228\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pdfplumber 0.11.5 requires pdfminer.six==20231228, but you have pdfminer-six 20240706 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed camelot-py-1.0.0 pdfminer-six-20240706 pypdf-3.17.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text and tables from the new meta-analysis PDF\n",
        "import pdfplumber\n",
        "\n",
        "# Path to the new uploaded PDF\n",
        "pdf_path = \"/content/ACt-vs-CBT-for-Anxiety.pdf\"\n",
        "\n",
        "# Initialize storage for text and tables\n",
        "extracted_text = []\n",
        "extracted_tables = []\n",
        "\n",
        "# Open the PDF and extract text and tables\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    for page in pdf.pages:\n",
        "        # Extract text from each page\n",
        "        text = page.extract_text()\n",
        "        if text:\n",
        "            extracted_text.append(text)\n",
        "\n",
        "        # Extract tables from each page\n",
        "        tables = page.extract_tables()\n",
        "        if tables:\n",
        "            extracted_tables.extend(tables)\n",
        "\n",
        "# Combine extracted text into one cleaned string\n",
        "cleaned_text = \"\\n\".join(extracted_text)\n",
        "\n",
        "# Save text as Markdown file\n",
        "md_file_path = \"/content/ACT_vs_CBT_cleaned.md\"\n",
        "with open(md_file_path, \"w\") as f:\n",
        "    f.write(cleaned_text)"
      ],
      "metadata": {
        "id": "VoRa-2Jd59-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import camelot\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Path to the uploaded PDF\n",
        "pdf_path = \"/content/ACt-vs-CBT-for-Anxiety.pdf\"\n",
        "\n",
        "# Extract tables using Camelot's \"stream\" mode\n",
        "tables = camelot.read_pdf(pdf_path, pages=\"all\", flavor=\"stream\")\n",
        "\n",
        "# Check how many tables were detected\n",
        "print(f\"Total tables detected: {tables.n}\")\n",
        "\n",
        "if tables.n > 0:\n",
        "    extracted_tables = [tables[i].df for i in range(tables.n)]\n",
        "    combined_table_df = pd.concat(extracted_tables, ignore_index=True)\n",
        "\n",
        "    # **Remove non-table content by setting a length threshold**\n",
        "    max_text_length = 100  # Adjust if needed\n",
        "    df_cleaned = combined_table_df[combined_table_df.apply(lambda row: row.astype(str).str.len().max() < max_text_length, axis=1)]\n",
        "\n",
        "    # Save cleaned tables as CSV\n",
        "    csv_file_path = \"/content/ACT_vs_CBT_cleaned_tables.csv\"\n",
        "    df_cleaned.to_csv(csv_file_path, index=False)\n",
        "\n",
        "    print(f\"Cleaned tables saved at: {csv_file_path}\")\n",
        "\n",
        "    # Download the cleaned CSV\n",
        "    files.download(csv_file_path)\n",
        "else:\n",
        "    print(\"No tables detected.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ZWFOIpk8ArYc",
        "outputId": "db98d118-9c39-4052-b680-d358abb61a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tables detected: 26\n",
            "Cleaned tables saved at: /content/ACT_vs_CBT_cleaned_tables.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ef7c6959-ee37-43a0-ad7f-fdd6c0a41c09\", \"ACT_vs_CBT_cleaned_tables.csv\", 49310)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from google.colab import files\n",
        "\n",
        "# Step 1: Load the cleaned CSV file (already extracted using Camelot)\n",
        "file_path = \"/content/ACT_vs_CBT_cleaned_tables.csv\"\n",
        "df = pd.read_csv(file_path, header=None, encoding='utf-8', on_bad_lines='skip')\n",
        "\n",
        "# Step 2: Remove rows containing unwanted text patterns\n",
        "unwanted_patterns = [\n",
        "    \"doi:\", \"DOI\", \"Retrieved from\", \"References\", \"Appendix\",\n",
        "    \"Supplemental materials\", \"This article was published\", \"E-mail\"\n",
        "]\n",
        "\n",
        "df = df[~df.apply(lambda row: row.astype(str).str.contains('|'.join(unwanted_patterns), case=False, na=False).any(), axis=1)]\n",
        "\n",
        "# Step 3: Filter out rows that are not structured tables\n",
        "def is_table_row(row):\n",
        "    \"\"\"Check if a row is likely part of a table (has at least 2 numbers).\"\"\"\n",
        "    text = ' '.join(map(str, row)).strip()\n",
        "    num_count = len(re.findall(r'\\d+', text))  # Count numeric values\n",
        "    return num_count >= 2  # Keep only rows with at least 2 numbers\n",
        "\n",
        "df_filtered = df[df.apply(is_table_row, axis=1)]\n",
        "\n",
        "# Step 4: Fix encoding issues and clean text\n",
        "def clean_text(text):\n",
        "    \"\"\"Fixes encoding issues and removes unwanted characters.\"\"\"\n",
        "    if isinstance(text, str):\n",
        "        text = text.replace(\"â€”\", \"—\")  # Fixes dashes\n",
        "        text = text.replace(\"\\n\", \" \")  # Removes line breaks\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()  # Normalize spaces\n",
        "    return text\n",
        "\n",
        "df_filtered = df_filtered.applymap(clean_text)\n",
        "\n",
        "# Step 5: Save the final cleaned table\n",
        "final_csv_path = \"/content/ACT_vs_CBT_cleaned_table_2.csv\"\n",
        "df_filtered.to_csv(final_csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"✅ Fully cleaned tables saved at: {final_csv_path}\")\n",
        "\n",
        "# Step 6: Provide a download link for the final cleaned CSV\n",
        "files.download(final_csv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "6AmrSE4nKEI6",
        "outputId": "080779d3-d177-4a46-ad6a-4cd0db6eafb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fully cleaned tables saved at: /content/ACT_vs_CBT_cleaned_table_2.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-09e1991c3131>:35: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df_filtered = df_filtered.applymap(clean_text)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ceffa0e5-6a6d-4e51-98a6-b626a0c59961\", \"ACT_vs_CBT_cleaned_table_2.csv\", 16230)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from google.colab import files\n",
        "\n",
        "# Load the cleaned CSV file\n",
        "file_path = \"/content/ACT_vs_CBT_cleaned_table_2.csv\"\n",
        "df = pd.read_csv(file_path, header=None, encoding='utf-8', on_bad_lines='skip')\n",
        "\n",
        "# Function to check if a row belongs to a table (at least 2 numeric values)\n",
        "def is_table_row(row):\n",
        "    \"\"\"Check if a row is part of a table by detecting at least 2 numbers.\"\"\"\n",
        "    text = ' '.join(map(str, row)).strip()\n",
        "    num_count = len(re.findall(r'\\d+', text))\n",
        "    return num_count >= 2  # True if at least 2 numbers exist\n",
        "\n",
        "# Create a boolean column to mark table rows\n",
        "df[\"is_table\"] = df.apply(is_table_row, axis=1)\n",
        "\n",
        "# Identify title rows (row before a table)\n",
        "df[\"is_title\"] = False  # Default to False\n",
        "table_indices = df.index[df[\"is_table\"]].tolist()\n",
        "for idx in table_indices:\n",
        "    if idx > 0:  # Avoid first-row issues\n",
        "        df.at[idx - 1, \"is_title\"] = True  # Mark title row\n",
        "\n",
        "# Keep only tables and their title rows\n",
        "df_filtered = df[df[\"is_table\"] | df[\"is_title\"]].drop(columns=[\"is_table\", \"is_title\"], errors=\"ignore\")\n",
        "\n",
        "# Save the cleaned CSV\n",
        "final_csv_path = \"/content/ACT_vs_CBT_cleaned_table_3.csv\"\n",
        "df_filtered.to_csv(final_csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"✅ Final structured tables saved at: {final_csv_path}\")\n",
        "\n",
        "# Download the cleaned CSV\n",
        "files.download(final_csv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "FHdmddrbNPGA",
        "outputId": "1351a55b-1b54-4a11-f3b2-5e3ee5eca262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Final structured tables saved at: /content/ACT_vs_CBT_cleaned_table_3.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ddd1683f-0214-472b-9730-cc71293fc2e0\", \"ACT_vs_CBT_cleaned_table_3.csv\", 16246)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "\n",
        "import pandas as pd\n",
        "import fitz  # PyMuPDF for PDF processing\n",
        "import re\n",
        "from google.colab import files\n",
        "\n",
        "# Upload the files manually or provide paths\n",
        "pdf_path = \"/content/ACt-vs-CBT-for-Anxiety.pdf\"\n",
        "csv_path = \"/content/ACT_vs_CBT_cleaned_table_3.csv\"  # Your cleaned CSV file\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(csv_path, header=None, encoding='utf-8', on_bad_lines='skip')\n",
        "\n",
        "# Extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    for page in doc:\n",
        "        text += page.get_text(\"text\") + \"\\n\"\n",
        "    return text\n",
        "\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Extract table titles using regex (assumes \"Table X: Title\" format)\n",
        "table_titles = re.findall(r\"(Table\\s\\d+:.*)\", pdf_text)\n",
        "\n",
        "# Function to check if a row is part of a table (at least 2 numeric values)\n",
        "def is_table_row(row):\n",
        "    text = ' '.join(map(str, row)).strip()\n",
        "    num_count = len(re.findall(r'\\d+', text))\n",
        "    return num_count >= 2  # True if at least 2 numbers exist\n",
        "\n",
        "# Detect table rows\n",
        "df[\"is_table\"] = df.apply(is_table_row, axis=1)\n",
        "\n",
        "# Identify potential title rows (row before a table)\n",
        "df[\"is_title\"] = False\n",
        "table_indices = df.index[df[\"is_table\"]].tolist()\n",
        "\n",
        "for i, idx in enumerate(table_indices):\n",
        "    if idx > 0 and i < len(table_titles):  # Ensure within bounds\n",
        "        df.at[idx - 1, 0] = table_titles[i]  # Assign title to first column of previous row\n",
        "\n",
        "# Remove helper columns\n",
        "df.drop(columns=[\"is_table\", \"is_title\"], errors=\"ignore\", inplace=True)\n",
        "\n",
        "# Save cleaned file\n",
        "final_csv_path = \"/content/ACT_vs_CBT_cleaned_table_4.csv\"\n",
        "df.to_csv(final_csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"✅ Final structured tables with titles saved at: {final_csv_path}\")\n",
        "\n",
        "# Download the cleaned CSV\n",
        "files.download(final_csv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "1uwA_fgxQy3g",
        "outputId": "7cc0e657-b8a5-4e4f-8a18-68182128ff7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.3)\n",
            "✅ Final structured tables with titles saved at: /content/ACT_vs_CBT_cleaned_table_4.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8d0491ae-ee68-4946-8839-55526db967e1\", \"ACT_vs_CBT_cleaned_table_4.csv\", 16262)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import camelot\n",
        "import pandas as pd\n",
        "import re\n",
        "from google.colab import files\n",
        "\n",
        "# 📌 Step 1: Extract Tables from the PDF\n",
        "pdf_path = \"/content/ACt-vs-CBT-for-Anxiety.pdf\"\n",
        "\n",
        "# Extract tables using Camelot (stream mode)\n",
        "tables = camelot.read_pdf(pdf_path, pages=\"all\", flavor=\"stream\")\n",
        "\n",
        "# Check how many tables were detected\n",
        "print(f\"Total tables detected: {tables.n}\")\n",
        "\n",
        "if tables.n > 0:\n",
        "    extracted_tables = [tables[i].df for i in range(tables.n)]\n",
        "    combined_table_df = pd.concat(extracted_tables, ignore_index=True)\n",
        "\n",
        "    # **Remove non-table content by setting a length threshold**\n",
        "    max_text_length = 100  # Adjust if needed\n",
        "    df_cleaned = combined_table_df[\n",
        "        combined_table_df.apply(lambda row: row.astype(str).str.len().max() < max_text_length, axis=1)\n",
        "    ]\n",
        "\n",
        "    # Save cleaned tables as CSV\n",
        "    csv_file_path = \"/content/ACT_vs_CBT_cleaned_tables_1.csv\"\n",
        "    df_cleaned.to_csv(csv_file_path, index=False)\n",
        "\n",
        "    print(f\"✅ Cleaned tables saved at: {csv_file_path}\")\n",
        "\n",
        "    # Download the cleaned CSV\n",
        "    files.download(csv_file_path)\n",
        "else:\n",
        "    print(\"❌ No tables detected.\")\n",
        "    exit()\n",
        "\n",
        "# 📌 Step 2: Load the Cleaned CSV and Filter Out Unwanted Text\n",
        "file_path = \"/content/ACT_vs_CBT_cleaned_tables_1.csv\"\n",
        "df = pd.read_csv(file_path, header=None, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
        "\n",
        "# Remove rows containing unwanted text patterns\n",
        "unwanted_patterns = [\n",
        "    \"doi:\", \"DOI\", \"Retrieved from\", \"References\", \"Appendix\",\n",
        "    \"Supplemental materials\", \"This article was published\", \"E-mail\"\n",
        "]\n",
        "\n",
        "df = df[\n",
        "    ~df.apply(lambda row: row.astype(str).str.contains(\"|\".join(unwanted_patterns), case=False, na=False).any(), axis=1)\n",
        "]\n",
        "\n",
        "# 📌 Step 3: Keep Only Table Rows (Require at Least 2 Numeric Values)\n",
        "def is_table_row(row):\n",
        "    \"\"\"Check if a row is likely part of a table (has at least 2 numbers).\"\"\"\n",
        "    text = \" \".join(map(str, row)).strip()\n",
        "    num_count = len(re.findall(r\"\\d+\", text))  # Count numeric values\n",
        "    return num_count >= 2  # Keep only rows with at least 2 numbers\n",
        "\n",
        "df_filtered = df[df.apply(is_table_row, axis=1)]\n",
        "\n",
        "# 📌 Step 4: Fix Encoding Issues and Clean Text\n",
        "def clean_text(text):\n",
        "    \"\"\"Fix encoding issues and remove unwanted characters.\"\"\"\n",
        "    if isinstance(text, str):\n",
        "        text = text.replace(\"â€”\", \"—\")  # Fixes dashes\n",
        "        text = text.replace(\"\\n\", \" \")  # Removes line breaks\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize spaces\n",
        "    return text\n",
        "\n",
        "df_filtered = df_filtered.applymap(clean_text)\n",
        "\n",
        "# 📌 Step 5: Save the Final Cleaned Table\n",
        "final_csv_path = \"/content/ACT_vs_CBT_cleaned_tables_2.csv\"\n",
        "df_filtered.to_csv(final_csv_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"✅ Fully cleaned tables saved at: {final_csv_path}\")\n",
        "\n",
        "# 📌 Step 6: Provide a Download Link for the Final Cleaned CSV\n",
        "files.download(final_csv_path)\n",
        "\n",
        "# 📌 Step 7: Load the Cleaned CSV Again for Final Processing\n",
        "file_path = \"/content/ACT_vs_CBT_cleaned_tables_2.csv\"\n",
        "df = pd.read_csv(file_path, header=None, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
        "\n",
        "# Function to check if a row belongs to a table (at least 2 numeric values)\n",
        "def is_table_row(row):\n",
        "    \"\"\"Check if a row is part of a table by detecting at least 2 numbers.\"\"\"\n",
        "    text = \" \".join(map(str, row)).strip()\n",
        "    num_count = len(re.findall(r\"\\d+\", text))\n",
        "    return num_count >= 2  # True if at least 2 numbers exist\n",
        "\n",
        "# Create a boolean column to mark table rows\n",
        "df[\"is_table\"] = df.apply(is_table_row, axis=1)\n",
        "\n",
        "# Identify title rows (row before a table)\n",
        "df[\"is_title\"] = False  # Default to False\n",
        "table_indices = df.index[df[\"is_table\"]].tolist()\n",
        "for idx in table_indices:\n",
        "    if idx > 0:  # Avoid first-row issues\n",
        "        df.at[idx - 1, \"is_title\"] = True  # Mark title row\n",
        "\n",
        "# Keep only tables and their title rows\n",
        "df_filtered = df[df[\"is_table\"] | df[\"is_title\"]].drop(columns=[\"is_table\", \"is_title\"], errors=\"ignore\")\n",
        "\n",
        "# 📌 Step 8: Save the Final Structured CSV\n",
        "final_csv_path = \"/content/ACT_vs_CBT_cleaned_tables_3.csv\"\n",
        "df_filtered.to_csv(final_csv_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"✅ Final structured tables saved at: {final_csv_path}\")\n",
        "\n",
        "# 📌 Step 9: Download the Final CSV\n",
        "files.download(final_csv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "AHF0U3Ff4uVJ",
        "outputId": "84ff63b5-e94c-4552-d57e-c807db2a4835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tables detected: 26\n",
            "✅ Cleaned tables saved at: /content/ACT_vs_CBT_cleaned_tables_1.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_440054eb-9dbe-4314-bea5-4f141a245d39\", \"ACT_vs_CBT_cleaned_tables_1.csv\", 49310)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fully cleaned tables saved at: /content/ACT_vs_CBT_cleaned_tables_2.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-b32f1471e16b>:69: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df_filtered = df_filtered.applymap(clean_text)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ae8ea572-98f9-477a-a25e-16b241eba04c\", \"ACT_vs_CBT_cleaned_tables_2.csv\", 16230)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Final structured tables saved at: /content/ACT_vs_CBT_cleaned_tables_3.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b361ffb4-8b26-4f05-a1f9-c47ec93aeaf9\", \"ACT_vs_CBT_cleaned_tables_3.csv\", 16246)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = \"/content/ACT_vs_CBT_cleaned_tables_3.csv\"\n",
        "\n",
        "# Read CSV while ignoring unnecessary rows at the start\n",
        "df = pd.read_csv(file_path, skip_blank_lines=True, header=None)\n",
        "\n",
        "# Drop rows where most columns contain non-numeric or long text (removes text-heavy noise)\n",
        "df_cleaned = df[~df.astype(str).apply(lambda x: x.str.contains(r'[a-zA-Z\\s]{25,}', regex=True)).any(axis=1)]\n",
        "\n",
        "# Drop rows where all values are NaN\n",
        "df_cleaned = df_cleaned.dropna(how='all')\n",
        "\n",
        "# Remove first 3 unwanted rows if they contain just \"0\" and \"1\" artifacts\n",
        "if df_cleaned.iloc[:3].applymap(lambda x: str(x).strip()).isin([\"0\", \"1\"]).all(axis=1).sum() == 3:\n",
        "    df_cleaned = df_cleaned.iloc[3:]\n",
        "\n",
        "# Reset index after cleaning\n",
        "df_cleaned.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "cleaned_file_path = \"/content/ACT_vs_CBT_final_cleaned.csv\"\n",
        "df_cleaned.to_csv(cleaned_file_path, index=False, header=False)\n",
        "\n",
        "print(f\"✅ Final cleaned CSV saved to: {cleaned_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W84Xcn21G-d-",
        "outputId": "7b0c5f21-0fb5-46be-ccad-36afd01a389e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Final cleaned CSV saved to: /content/ACT_vs_CBT_final_cleaned.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-12a8089bbcf7>:16: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  if df_cleaned.iloc[:3].applymap(lambda x: str(x).strip()).isin([\"0\", \"1\"]).all(axis=1).sum() == 3:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "pdf_dir = \"/content/drive/My Drive/4th SEM Project/Meta Papers\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "H7ZvouKsSnOk",
        "outputId": "608f3f56-c80d-4975-adc6-fd462ccf6c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-59616e389171>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpdf_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/4th SEM Project/Meta Papers\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    }
  ]
}