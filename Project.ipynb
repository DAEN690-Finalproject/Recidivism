{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing CHFC7F_1 (Developer 4/CHFC7F_1/CHFC7F_1.pdf)...\n",
      "‚úÖ Extracted text saved to: Developer 4/CHFC7F_1\\CHFC7F_1_extracted_text.txt\n",
      "‚úÖ Extracted table data saved to: Developer 4/CHFC7F_1\\CHFC7F_1_extracted_tables.csv\n",
      "\n",
      "Processing Chapter_15_Summary (Developer 4/Chapter 15 Summary & Conclusions/Chapter 15 Summary & Conclusions.pdf)...\n",
      "‚úÖ Extracted text saved to: Developer 4/Chapter 15 Summary & Conclusions\\Chapter_15_Summary_extracted_text.txt\n",
      "‚úÖ Extracted table data saved to: Developer 4/Chapter 15 Summary & Conclusions\\Chapter_15_Summary_extracted_tables.csv\n",
      "\n",
      "Processing CorrectionalEducationandRecidivism (Developer 4/CorrectionalEducationandRecidivism-TowardAToolforReduction/CorrectionalEducationandRecidivism-TowardAToolforReduction.pdf)...\n",
      "‚úÖ Extracted text saved to: Developer 4/CorrectionalEducationandRecidivism-TowardAToolforReduction\\CorrectionalEducationandRecidivism_extracted_text.txt\n",
      "‚úÖ Extracted table data saved to: Developer 4/CorrectionalEducationandRecidivism-TowardAToolforReduction\\CorrectionalEducationandRecidivism_extracted_tables.csv\n",
      "\n",
      "Processing Dodson_et_al_2011-libre (Developer 4/Dodson_et_al_2011-libre/Dodson_et_al_2011-libre.pdf)...\n",
      "‚úÖ Extracted text saved to: Developer 4/Dodson_et_al_2011-libre\\Dodson_et_al_2011-libre_extracted_text.txt\n",
      "‚úÖ Extracted table data saved to: Developer 4/Dodson_et_al_2011-libre\\Dodson_et_al_2011-libre_extracted_tables.csv\n",
      "\n",
      "Processing Does_incarceration_based_drug_treatment (Developer 4/Does_incarceration_based_drug_treatment (1)/Does_incarceration_based_drug_treatment (1).pdf)...\n",
      "‚úÖ Extracted text saved to: Developer 4/Does_incarceration_based_drug_treatment (1)\\Does_incarceration_based_drug_treatment_extracted_text.txt\n",
      "‚ö†Ô∏è No tables found using pdfplumber in Developer 4/Does_incarceration_based_drug_treatment (1)/Does_incarceration_based_drug_treatment (1).pdf\n",
      "üîÑ Trying alternative table extraction for Does_incarceration_based_drug_treatment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17T19:40:39 - INFO - Processing page-1\n",
      "2025-03-17 19:40:39,145 - INFO - Processing page-1\n",
      "2025-03-17T19:40:39 - INFO - Processing page-2\n",
      "2025-03-17 19:40:39,479 - INFO - Processing page-2\n",
      "2025-03-17T19:40:40 - INFO - Processing page-3\n",
      "2025-03-17 19:40:40,058 - INFO - Processing page-3\n",
      "2025-03-17T19:40:40 - INFO - Processing page-4\n",
      "2025-03-17 19:40:40,683 - INFO - Processing page-4\n",
      "2025-03-17T19:40:41 - INFO - Processing page-5\n",
      "2025-03-17 19:40:41,325 - INFO - Processing page-5\n",
      "2025-03-17T19:40:42 - INFO - Processing page-6\n",
      "2025-03-17 19:40:42,207 - INFO - Processing page-6\n",
      "2025-03-17T19:40:43 - INFO - Processing page-7\n",
      "2025-03-17 19:40:43,093 - INFO - Processing page-7\n",
      "2025-03-17T19:40:43 - INFO - Processing page-8\n",
      "2025-03-17 19:40:43,725 - INFO - Processing page-8\n",
      "2025-03-17T19:40:44 - INFO - Processing page-9\n",
      "2025-03-17 19:40:44,332 - INFO - Processing page-9\n",
      "2025-03-17T19:40:44 - INFO - Processing page-10\n",
      "2025-03-17 19:40:44,661 - INFO - Processing page-10\n",
      "2025-03-17T19:40:45 - INFO - Processing page-11\n",
      "2025-03-17 19:40:45,190 - INFO - Processing page-11\n",
      "2025-03-17T19:40:45 - INFO - Processing page-12\n",
      "2025-03-17 19:40:45,550 - INFO - Processing page-12\n",
      "2025-03-17T19:40:46 - INFO - Processing page-13\n",
      "2025-03-17 19:40:46,025 - INFO - Processing page-13\n",
      "2025-03-17T19:40:46 - INFO - Processing page-14\n",
      "2025-03-17 19:40:46,489 - INFO - Processing page-14\n",
      "2025-03-17T19:40:46 - INFO - Processing page-15\n",
      "2025-03-17 19:40:46,744 - INFO - Processing page-15\n",
      "2025-03-17T19:40:46 - INFO - Processing page-16\n",
      "2025-03-17 19:40:46,960 - INFO - Processing page-16\n",
      "2025-03-17T19:40:47 - INFO - Processing page-17\n",
      "2025-03-17 19:40:47,361 - INFO - Processing page-17\n",
      "2025-03-17T19:40:48 - INFO - Processing page-18\n",
      "2025-03-17 19:40:48,285 - INFO - Processing page-18\n",
      "2025-03-17T19:40:49 - INFO - Processing page-19\n",
      "2025-03-17 19:40:49,052 - INFO - Processing page-19\n",
      "2025-03-17T19:40:49 - INFO - Processing page-20\n",
      "2025-03-17 19:40:49,922 - INFO - Processing page-20\n",
      "2025-03-17T19:40:50 - INFO - Processing page-21\n",
      "2025-03-17 19:40:50,309 - INFO - Processing page-21\n",
      "2025-03-17T19:40:50 - INFO - Processing page-22\n",
      "2025-03-17 19:40:50,673 - INFO - Processing page-22\n",
      "2025-03-17T19:40:50 - INFO - Processing page-23\n",
      "2025-03-17 19:40:50,910 - INFO - Processing page-23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted tables saved using Camelot to: Developer 4/Does_incarceration_based_drug_treatment (1)\\Does_incarceration_based_drug_treatment_camelot_tables.csv\n",
      "\n",
      "Processing drug_court_metaanalysis (Developer 4/drug court metaanalysis/drug court metaanalysis.pdf)...\n",
      "‚úÖ Extracted text saved to: Developer 4/drug court metaanalysis\\drug_court_metaanalysis_extracted_text.txt\n",
      "‚úÖ Extracted table data saved to: Developer 4/drug court metaanalysis\\drug_court_metaanalysis_extracted_tables.csv\n",
      "\n",
      "Processing education_metaanalysis (Developer 4/education metaanlysis/education metaanlysis.pdf)...\n",
      "‚úÖ Extracted text saved to: Developer 4/education metaanlysis\\education_metaanalysis_extracted_text.txt\n",
      "‚úÖ Extracted table data saved to: Developer 4/education metaanlysis\\education_metaanalysis_extracted_tables.csv\n",
      "\n",
      "Processing Meta-analysis-of-CBT (Developer 4/Meta-analysis-of-CBT-Landenberger-Lipsey_CBT_JEC-paper/Meta-analysis-of-CBT-Landenberger-Lipsey_CBT_JEC-paper.pdf)...\n",
      "‚úÖ Extracted text saved to: Developer 4/Meta-analysis-of-CBT-Landenberger-Lipsey_CBT_JEC-paper\\Meta-analysis-of-CBT_extracted_text.txt\n",
      "‚ö†Ô∏è No tables found using pdfplumber in Developer 4/Meta-analysis-of-CBT-Landenberger-Lipsey_CBT_JEC-paper/Meta-analysis-of-CBT-Landenberger-Lipsey_CBT_JEC-paper.pdf\n",
      "üîÑ Trying alternative table extraction for Meta-analysis-of-CBT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17T19:41:23 - INFO - Processing page-1\n",
      "2025-03-17 19:41:23,477 - INFO - Processing page-1\n",
      "2025-03-17T19:41:23 - INFO - Processing page-2\n",
      "2025-03-17 19:41:23,711 - INFO - Processing page-2\n",
      "2025-03-17T19:41:24 - INFO - Processing page-3\n",
      "2025-03-17 19:41:24,484 - INFO - Processing page-3\n",
      "2025-03-17T19:41:25 - INFO - Processing page-4\n",
      "2025-03-17 19:41:25,229 - INFO - Processing page-4\n",
      "2025-03-17T19:41:25 - INFO - Processing page-5\n",
      "2025-03-17 19:41:25,885 - INFO - Processing page-5\n",
      "2025-03-17T19:41:26 - INFO - Processing page-6\n",
      "2025-03-17 19:41:26,190 - INFO - Processing page-6\n",
      "2025-03-17T19:41:26 - INFO - Processing page-7\n",
      "2025-03-17 19:41:26,301 - INFO - Processing page-7\n",
      "2025-03-17T19:41:26 - INFO - Processing page-8\n",
      "2025-03-17 19:41:26,385 - INFO - Processing page-8\n",
      "2025-03-17T19:41:26 - INFO - Processing page-9\n",
      "2025-03-17 19:41:26,456 - INFO - Processing page-9\n",
      "2025-03-17T19:41:26 - INFO - Processing page-10\n",
      "2025-03-17 19:41:26,678 - INFO - Processing page-10\n",
      "2025-03-17T19:41:27 - INFO - Processing page-11\n",
      "2025-03-17 19:41:27,012 - INFO - Processing page-11\n",
      "2025-03-17T19:41:27 - INFO - Processing page-12\n",
      "2025-03-17 19:41:27,314 - INFO - Processing page-12\n",
      "2025-03-17T19:41:27 - INFO - Processing page-13\n",
      "2025-03-17 19:41:27,600 - INFO - Processing page-13\n",
      "2025-03-17T19:41:27 - INFO - Processing page-14\n",
      "2025-03-17 19:41:27,753 - INFO - Processing page-14\n",
      "2025-03-17T19:41:28 - INFO - Processing page-15\n",
      "2025-03-17 19:41:28,004 - INFO - Processing page-15\n",
      "2025-03-17T19:41:28 - INFO - Processing page-16\n",
      "2025-03-17 19:41:28,280 - INFO - Processing page-16\n",
      "2025-03-17T19:41:28 - INFO - Processing page-17\n",
      "2025-03-17 19:41:28,408 - INFO - Processing page-17\n",
      "2025-03-17T19:41:28 - INFO - Processing page-18\n",
      "2025-03-17 19:41:28,550 - INFO - Processing page-18\n",
      "2025-03-17T19:41:28 - INFO - Processing page-19\n",
      "2025-03-17 19:41:28,811 - INFO - Processing page-19\n",
      "2025-03-17T19:41:29 - INFO - Processing page-20\n",
      "2025-03-17 19:41:29,215 - INFO - Processing page-20\n",
      "2025-03-17T19:41:29 - INFO - Processing page-21\n",
      "2025-03-17 19:41:29,761 - INFO - Processing page-21\n",
      "2025-03-17T19:41:30 - INFO - Processing page-22\n",
      "2025-03-17 19:41:30,373 - INFO - Processing page-22\n",
      "2025-03-17T19:41:31 - INFO - Processing page-23\n",
      "2025-03-17 19:41:31,097 - INFO - Processing page-23\n",
      "2025-03-17T19:41:31 - INFO - Processing page-24\n",
      "2025-03-17 19:41:31,590 - INFO - Processing page-24\n",
      "2025-03-17T19:41:32 - INFO - Processing page-25\n",
      "2025-03-17 19:41:32,188 - INFO - Processing page-25\n",
      "2025-03-17T19:41:32 - INFO - Processing page-26\n",
      "2025-03-17 19:41:32,737 - INFO - Processing page-26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted tables saved using Camelot to: Developer 4/Meta-analysis-of-CBT-Landenberger-Lipsey_CBT_JEC-paper\\Meta-analysis-of-CBT_camelot_tables.csv\n",
      "\n",
      "Processing CPA_Study (Developer 4/CPA Study 2015/CPA Study 2015.pdf)...\n",
      "‚úÖ Successfully processed IEEE format PDF Developer 4/CPA Study 2015/CPA Study 2015.pdf\n",
      "   - Full text saved to: Developer 4/CPA Study 2015\\CPA Study 2015_full_text.txt\n",
      "   - Structured report saved to: Developer 4/CPA Study 2015\\CPA Study 2015_structured_report.txt\n",
      "\n",
      "Processing Mackinac (Developer 4/Mackinac I/Mackinac I.pdf)...\n",
      "‚úÖ Successfully processed IEEE format PDF Developer 4/Mackinac I/Mackinac I.pdf\n",
      "   - Full text saved to: Developer 4/Mackinac I\\Mackinac I_full_text.txt\n",
      "   - Structured report saved to: Developer 4/Mackinac I\\Mackinac I_structured_report.txt\n",
      "\n",
      "=== PROCESSING COMPLETE ===\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for text extraction\n",
    "import pdfplumber  # For basic table extraction\n",
    "import camelot  # For structured table extraction\n",
    "from tabula import convert_into  # For backup table extraction\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Dictionary containing all PDFs and their paths\n",
    "pdf_files = {\n",
    "    # Standard PDFs\n",
    "    \"CHFC7F_1\": r\"Developer 4/CHFC7F_1/CHFC7F_1.pdf\",\n",
    "    \"Chapter_15_Summary\": r\"Developer 4/Chapter 15 Summary & Conclusions/Chapter 15 Summary & Conclusions.pdf\",\n",
    "    \"CorrectionalEducationandRecidivism\": r\"Developer 4/CorrectionalEducationandRecidivism-TowardAToolforReduction/CorrectionalEducationandRecidivism-TowardAToolforReduction.pdf\",\n",
    "    \"Dodson_et_al_2011-libre\": r\"Developer 4/Dodson_et_al_2011-libre/Dodson_et_al_2011-libre.pdf\",\n",
    "    \"Does_incarceration_based_drug_treatment\": r\"Developer 4/Does_incarceration_based_drug_treatment (1)/Does_incarceration_based_drug_treatment (1).pdf\",\n",
    "    \"drug_court_metaanalysis\": r\"Developer 4/drug court metaanalysis/drug court metaanalysis.pdf\",\n",
    "    \"education_metaanalysis\": r\"Developer 4/education metaanlysis/education metaanlysis.pdf\",\n",
    "    \"Meta-analysis-of-CBT\": r\"Developer 4/Meta-analysis-of-CBT-Landenberger-Lipsey_CBT_JEC-paper/Meta-analysis-of-CBT-Landenberger-Lipsey_CBT_JEC-paper.pdf\",\n",
    "    \n",
    "    # IEEE format PDFs\n",
    "    \"CPA_Study\": r\"Developer 4/CPA Study 2015/CPA Study 2015.pdf\",\n",
    "    \"Mackinac\": r\"Developer 4/Mackinac I/Mackinac I.pdf\"\n",
    "}\n",
    "\n",
    "# PDFs requiring an alternative table extraction method\n",
    "alternative_table_pdfs = [\n",
    "    \"Does_incarceration_based_drug_treatment\",\n",
    "    \"Meta-analysis-of-CBT\"\n",
    "]\n",
    "\n",
    "# IEEE format PDFs\n",
    "ieee_format_pdfs = [\n",
    "    \"CPA_Study\",\n",
    "    \"Mackinac\"\n",
    "]\n",
    "\n",
    "def extract_metadata(text):\n",
    "    \"\"\"Extract metadata from IEEE format papers\"\"\"\n",
    "    metadata = {}\n",
    "    \n",
    "    # Try to extract title (usually at the beginning, often in larger font)\n",
    "    title_match = re.search(r'^(.*?)(?:\\n|Introduction)', text, re.IGNORECASE | re.DOTALL)\n",
    "    if title_match:\n",
    "        title = title_match.group(1).strip()\n",
    "        # Clean up multi-line titles\n",
    "        title = re.sub(r'\\n+', ' ', title)\n",
    "        metadata['title'] = title\n",
    "    \n",
    "    # Try to extract authors\n",
    "    author_pattern = r'By\\s+([A-Za-z\\s.,]+)'\n",
    "    author_match = re.search(author_pattern, text[:1000])  # Look in first 1000 chars\n",
    "    if author_match:\n",
    "        metadata['authors'] = author_match.group(1).strip()\n",
    "    \n",
    "    # Try to extract date\n",
    "    date_pattern = r'([A-Z][a-z]+\\.?\\s+\\d{1,2},\\s+\\d{4})'\n",
    "    date_match = re.search(date_pattern, text[:1000])\n",
    "    if date_match:\n",
    "        metadata['date'] = date_match.group(1)\n",
    "    \n",
    "    # Try to extract abstract (often after title but before introduction)\n",
    "    abstract_match = re.search(r'(?:Abstract|Introduction)(.*?)(?:Introduction|\\n\\n)', text, re.IGNORECASE | re.DOTALL)\n",
    "    if abstract_match:\n",
    "        metadata['abstract'] = abstract_match.group(1).strip()\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def extract_sections(text):\n",
    "    \"\"\"Extract major sections from the paper\"\"\"\n",
    "    # Common section headers in IEEE papers\n",
    "    section_patterns = [\n",
    "        r'Introduction', r'Methods?', r'Results?', r'Discussion', \n",
    "        r'Conclusion', r'References', r'Endnotes', r'Future Research',\n",
    "        r'Recommendations', r'Program Description', r'Evaluation Method'\n",
    "    ]\n",
    "    \n",
    "    # Create regex pattern to find these sections\n",
    "    pattern = '|'.join([f\"({p})\" for p in section_patterns])\n",
    "    \n",
    "    # Find all section headers and their positions\n",
    "    sections = {}\n",
    "    for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "        section_name = match.group(0)\n",
    "        start_pos = match.start()\n",
    "        sections[section_name] = {'start': start_pos}\n",
    "    \n",
    "    # Sort sections by their position in the text\n",
    "    sorted_sections = sorted(sections.items(), key=lambda x: x[1]['start'])\n",
    "    \n",
    "    # Extract content for each section\n",
    "    extracted_sections = {}\n",
    "    for i, (section_name, pos) in enumerate(sorted_sections):\n",
    "        start = pos['start']\n",
    "        if i < len(sorted_sections) - 1:\n",
    "            end = sorted_sections[i+1][1]['start']\n",
    "        else:\n",
    "            end = len(text)\n",
    "        \n",
    "        content = text[start:end].strip()\n",
    "        extracted_sections[section_name] = content\n",
    "    \n",
    "    return extracted_sections\n",
    "\n",
    "def extract_tables_from_text(text):\n",
    "    \"\"\"Extract tables based on patterns in the text\"\"\"\n",
    "    # Look for table-like structures with consistent patterns\n",
    "    tables = []\n",
    "    \n",
    "    # Try to find tables with headers like \"Table X\" or \"Graphic X\"\n",
    "    table_patterns = [\n",
    "        r'(Table\\s+\\d+.*?(?:\\n\\n|\\Z))',\n",
    "        r'(Graphic\\s+\\d+.*?(?:\\n\\n|\\Z))'\n",
    "    ]\n",
    "    \n",
    "    for pattern in table_patterns:\n",
    "        for match in re.finditer(pattern, text, re.DOTALL):\n",
    "            tables.append(match.group(1))\n",
    "    \n",
    "    return tables\n",
    "\n",
    "def extract_references(text):\n",
    "    \"\"\"Extract references or endnotes\"\"\"\n",
    "    references = []\n",
    "    \n",
    "    # Look for references or endnotes section\n",
    "    ref_section = None\n",
    "    for section_name in ['References', 'Endnotes']:\n",
    "        ref_pattern = f\"{section_name}(.*?)(?:\\n\\n\\w|\\Z)\"\n",
    "        ref_match = re.search(ref_pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "        if ref_match:\n",
    "            ref_section = ref_match.group(1).strip()\n",
    "            break\n",
    "    \n",
    "    if ref_section:\n",
    "        # Try to separate individual references\n",
    "        # Most common format is numbered references like \"1. Author...\"\n",
    "        ref_items = re.findall(r'\\d+\\.\\s+(.*?)(?=\\d+\\.|\\Z)', ref_section + \"999.\", re.DOTALL)\n",
    "        if ref_items:\n",
    "            references = [item.strip() for item in ref_items]\n",
    "        else:\n",
    "            # Alternative attempt - split by newlines for different format\n",
    "            references = [line.strip() for line in ref_section.split('\\n') if line.strip()]\n",
    "    \n",
    "    return references\n",
    "\n",
    "def process_ieee_pdf(pdf_path, output_dir):\n",
    "    \"\"\"Process an IEEE format PDF file and extract structured information\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract filename without extension\n",
    "    filename = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    \n",
    "    try:\n",
    "        # Open the PDF\n",
    "        doc = fitz.open(pdf_path)\n",
    "        \n",
    "        # Extract all text\n",
    "        full_text = \"\"\n",
    "        for page in doc:\n",
    "            full_text += page.get_text(\"text\") + \"\\n\"\n",
    "        \n",
    "        # Save the full text\n",
    "        text_output_path = os.path.join(output_dir, f\"{filename}_full_text.txt\")\n",
    "        with open(text_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(full_text)\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = extract_metadata(full_text)\n",
    "        \n",
    "        # Extract sections\n",
    "        sections = extract_sections(full_text)\n",
    "        \n",
    "        # Extract tables\n",
    "        tables = extract_tables_from_text(full_text)\n",
    "        \n",
    "        # Extract references\n",
    "        references = extract_references(full_text)\n",
    "        \n",
    "        # Save structured data\n",
    "        structured_output = {\n",
    "            \"metadata\": metadata,\n",
    "            \"sections\": sections,\n",
    "            \"tables\": tables,\n",
    "            \"references\": references\n",
    "        }\n",
    "        \n",
    "        # Save structured data as a text report\n",
    "        report_path = os.path.join(output_dir, f\"{filename}_structured_report.txt\")\n",
    "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"=== EXTRACTED METADATA ===\\n\")\n",
    "            for key, value in metadata.items():\n",
    "                f.write(f\"{key}: {value}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"\\n=== EXTRACTED SECTIONS ===\\n\")\n",
    "            for name, content in sections.items():\n",
    "                f.write(f\"## {name} ##\\n\")\n",
    "                f.write(f\"{content}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"\\n=== EXTRACTED TABLES ===\\n\")\n",
    "            for i, table in enumerate(tables):\n",
    "                f.write(f\"Table {i+1}:\\n{table}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"\\n=== EXTRACTED REFERENCES ===\\n\")\n",
    "            for i, ref in enumerate(references):\n",
    "                f.write(f\"{i+1}. {ref}\\n\")\n",
    "        \n",
    "        print(f\"‚úÖ Successfully processed IEEE format PDF {pdf_path}\")\n",
    "        print(f\"   - Full text saved to: {text_output_path}\")\n",
    "        print(f\"   - Structured report saved to: {report_path}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing IEEE format PDF {pdf_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def process_standard_pdf(name, pdf_path, output_dir):\n",
    "    \"\"\"Process a standard PDF file with text and table extraction\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Paths for output files\n",
    "    text_output_path = os.path.join(output_dir, f\"{name}_extracted_text.txt\")\n",
    "    csv_output_path = os.path.join(output_dir, f\"{name}_extracted_tables.csv\")\n",
    "\n",
    "    ### Extract text from PDF ###\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "\n",
    "        # Save extracted text\n",
    "        with open(text_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"‚úÖ Extracted text saved to: {text_output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting text from {pdf_path}: {e}\")\n",
    "\n",
    "    ### Extract tables from PDF ###\n",
    "    tables = []\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                table = page.extract_table()\n",
    "                if table:\n",
    "                    df = pd.DataFrame(table)\n",
    "                    tables.append(df)\n",
    "\n",
    "        # Save tables to CSV\n",
    "        if tables:\n",
    "            final_df = pd.concat(tables, ignore_index=True)\n",
    "            final_df.to_csv(csv_output_path, index=False, header=False)\n",
    "            print(f\"‚úÖ Extracted table data saved to: {csv_output_path}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No tables found using pdfplumber in {pdf_path}\")\n",
    "\n",
    "            # Use alternative method if the PDF is in the special list\n",
    "            if name in alternative_table_pdfs:\n",
    "                print(f\"üîÑ Trying alternative table extraction for {name}...\")\n",
    "\n",
    "                # Try Camelot first (works best for structured tables)\n",
    "                try:\n",
    "                    tables = camelot.read_pdf(pdf_path, pages=\"all\", flavor=\"stream\")\n",
    "                    if tables.n > 0:\n",
    "                        camelot_csv_path = os.path.join(output_dir, f\"{name}_camelot_tables.csv\")\n",
    "                        tables.export(camelot_csv_path, f=\"csv\", compress=False)\n",
    "                        print(f\"‚úÖ Extracted tables saved using Camelot to: {camelot_csv_path}\")\n",
    "                        return True\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Camelot failed for {name}: {e}\")\n",
    "\n",
    "                # If Camelot fails, use Tabula\n",
    "                try:\n",
    "                    tabula_csv_path = os.path.join(output_dir, f\"{name}_tabula_tables.csv\")\n",
    "                    convert_into(pdf_path, tabula_csv_path, output_format=\"csv\", pages=\"all\")\n",
    "                    print(f\"‚úÖ Extracted tables saved using Tabula to: {tabula_csv_path}\")\n",
    "                    return True\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Tabula also failed for {name}: {e}\")\n",
    "                    return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting tables from {pdf_path}: {e}\")\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    # Process each PDF\n",
    "    for name, pdf_path in pdf_files.items():\n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"‚ö†Ô∏è File not found: {pdf_path}\")\n",
    "            continue  # Skip this file if it doesn't exist\n",
    "\n",
    "        # Set output directory\n",
    "        output_dir = os.path.dirname(pdf_path)\n",
    "        \n",
    "        print(f\"\\nProcessing {name} ({pdf_path})...\")\n",
    "        \n",
    "        # Process based on PDF type\n",
    "        if name in ieee_format_pdfs:\n",
    "            process_ieee_pdf(pdf_path, output_dir)\n",
    "        else:\n",
    "            process_standard_pdf(name, pdf_path, output_dir)\n",
    "    \n",
    "    print(\"\\n=== PROCESSING COMPLETE ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
